{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Home Website Scarping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to know what social media each company have.\n",
    "\n",
    "By using the website link we scraped from YAHOO! Finance, we go to each website to see what social media it has and what's the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the list of public companies information scrapped from YAHOO! Finance\n",
    "df = pd.read_csv('listing.csv', index_col=\"'Unnamed: 0'\")\n",
    "df['facebook'] = None\n",
    "df['twitter'] = None\n",
    "df['linkedin'] = None\n",
    "df['youtube'] = None\n",
    "df['instagram'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap using request to get first version\n",
    "for i in df.index:\n",
    "    url = df['website'].loc[i]\n",
    "    print((i, url))\n",
    "    agent = {\"User-Agent\":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.87 Safari/537.36'}\n",
    "    cookies = {\"cookie\":\"COPY_HERE_YOUR_COOKIE_FROM_BROWSER\"}\n",
    "    try:\n",
    "        html = requests.get(url, headers=agent, cookies=cookies)\n",
    "        response = requests.get(html.url, headers=agent, cookies=cookies)\n",
    "        if html.status_code == 200:\n",
    "            soup = bs4.BeautifulSoup(html.content,\"html.parser\")\n",
    "            try:\n",
    "                facebook = soup.find(href=re.compile('facebook')).get('href')\n",
    "                df['facebook'].loc[i] = facebook\n",
    "            except:\n",
    "                print('no facebook')\n",
    "            try:\n",
    "                twitter = soup.find(href=re.compile('twitter')).get('href')\n",
    "                df['twitter'].loc[i] = twitter\n",
    "            except:\n",
    "                print('no twitter')\n",
    "            try:\n",
    "                linkedin = soup.find(href=re.compile('linkedin')).get('href')\n",
    "                df['linkedin'].loc[i] = linkedin\n",
    "            except:\n",
    "                print('no linkedin')\n",
    "            try:\n",
    "                instagram = soup.find(href=re.compile('instagram')).get('href')\n",
    "                df['instagram'].loc[i] = instagram\n",
    "            except:\n",
    "                print('no instagram')\n",
    "            try:\n",
    "                youtube = soup.find(href=re.compile('youtube')).get('href')\n",
    "                df['youtube'].loc[i] = youtube\n",
    "            except:\n",
    "                print('no youtube')\n",
    "        else:\n",
    "            print('forbidden')\n",
    "    except:\n",
    "        print('request failed')\n",
    "# df.to_csv(r'/Users/darren/Desktop/media.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are many websites that request can't get all the information, so use selenium to update the information\n",
    "media = pd.read_csv(\"update.csv\")\n",
    "# get the data that doesn't scrap from 1st version\n",
    "nomedia = media[media['facebook'].isnull() & media['twitter'].isnull() & media['linkedin'].isnull() &\n",
    "     media['youtube'].isnull() & media['instagram'].isnull()]\n",
    "# set the driver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "# scrap using selenium to get the second version\n",
    "count = 1\n",
    "for i in nomedia.index:\n",
    "    url = nomedia['website'].loc[i]\n",
    "    agent = {\"User-Agent\":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.87 Safari/537.36'}\n",
    "    cookies = {\"cookie\":\"COPY_HERE_YOUR_COOKIE_FROM_BROWSER\"}\n",
    "    print(count, url)\n",
    "    try:\n",
    "        html = requests.get(url, headers=agent, cookies=cookies, verify=False)\n",
    "        driver.get(html.url)\n",
    "        try:\n",
    "            facebook = driver.find_element_by_xpath(\"*//a[contains(@href,'facebook')]\").get_attribute('href')\n",
    "            nomedia['facebook'].loc[i] = facebook\n",
    "        except:\n",
    "            print('no facebook')\n",
    "        try:\n",
    "            twitter = driver.find_element_by_xpath(\"*//a[contains(@href,'twitter')]\").get_attribute('href')\n",
    "            nomedia['twitter'].loc[i] = twitter\n",
    "        except:\n",
    "            print('no twitter')\n",
    "        try:\n",
    "            linkedin = driver.find_element_by_xpath(\"*//a[contains(@href,'linkedin')]\").get_attribute('href')\n",
    "            nomedia['linkedin'].loc[i] = linkedin\n",
    "        except:\n",
    "            print('no linkedin')\n",
    "        try:\n",
    "            youtube = driver.find_element_by_xpath(\"*//a[contains(@href,'youtube')]\").get_attribute('href')\n",
    "            nomedia['youtube'].loc[i] = youtube\n",
    "        except:\n",
    "            print('no youtube')\n",
    "        try:\n",
    "            instagram = driver.find_element_by_xpath(\"*//a[contains(@href,'instagram')]\").get_attribute('href')\n",
    "            nomedia['instagram'].loc[i] = instagram\n",
    "        except:\n",
    "            print('no instagram')\n",
    "    except:\n",
    "        print('request failed')\n",
    "    count += 1\n",
    "new = nomedia[~(nomedia['facebook'].isnull() & nomedia['twitter'].isnull() & nomedia['linkedin'].isnull() &\n",
    "     nomedia['youtube'].isnull() & nomedia['instagram'].isnull())]\n",
    "# update with 2nd version\n",
    "for i in new.index:\n",
    "    media['facebook'][i] = new['facebook'][i]\n",
    "    media['twitter'][i] = new['twitter'][i]\n",
    "    media['linkedin'][i] = new['linkedin'][i]\n",
    "    media['youtube'][i] = new['youtube'][i]\n",
    "    media['instagram'][i] = new['instagram'][i]\n",
    "# show the new scraping results\n",
    "media[media['facebook'].isnull() & media['twitter'].isnull() & media['linkedin'].isnull() &\n",
    "     media['youtube'].isnull() & media['instagram'].isnull()]  \n",
    "# media.to_csv(r'/Users/darren/Desktop/latest.csv', index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
