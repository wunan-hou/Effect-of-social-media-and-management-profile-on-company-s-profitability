{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia API can give us the meta data of each company's wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wikipedia\n",
    "import wptools\n",
    "import requests\n",
    "import datetime\n",
    "import numpy as np\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of comapanies basic information\n",
    "df = pd.read_csv('latest.csv')\n",
    "wikis = dict()\n",
    "# start the API session\n",
    "S = requests.Session()\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "# scrap data from wikipedia API\n",
    "for i in df.index:\n",
    "    wiki = dict()\n",
    "    search = wikipedia.search(df['name'][i])\n",
    "    try:\n",
    "        # search if this company has a wikipedia page or not and get the wikipedia page\n",
    "        page = wptools.page(search[0])\n",
    "        print(i, \"wiki\")\n",
    "        query = wptools.page(search[0]).get_query()\n",
    "        if df['name'][i].lower().split()[0] == query.data['title'].lower().split()[0]:\n",
    "            # company information\n",
    "            try:\n",
    "                wiki['wikipedia'] = query.data['url']\n",
    "            except:\n",
    "                raise\n",
    "            try:\n",
    "                wiki['wiki_summary'] = query.data['extext']\n",
    "            except:\n",
    "                raise\n",
    "            # meta information\n",
    "            # page size\n",
    "            PARAMS1 = {\n",
    "                \"format\": \"json\",\n",
    "                \"action\": \"query\",\n",
    "                \"prop\": \"info\",\n",
    "                \"titles\": search[0]\n",
    "            }\n",
    "            info = S.get(url=URL, params=PARAMS1).json()\n",
    "            try:\n",
    "                page_id = list(info['query']['pages'].keys())[0]\n",
    "                wiki['page_size_bytes'] = info['query']['pages'][page_id]['length']\n",
    "            except:\n",
    "                raise\n",
    "            # edits\n",
    "            PARAMS2 = {\n",
    "                \"format\": \"json\",\n",
    "                \"action\": \"query\",\n",
    "                \"prop\": \"revisions\",\n",
    "                \"titles\": search[0],\n",
    "                \"rvprop\": \"ids|timestamp|size|userid|comment\",\n",
    "                \"rvlimit\": 500\n",
    "            }\n",
    "            revisions = S.get(url=URL, params=PARAMS2).json()\n",
    "            try:\n",
    "                edits = pd.DataFrame(revisions['query']['pages'][page_id]['revisions'])\n",
    "                for index in edits.index:\n",
    "                    edits['timestamp'][index] = datetime.datetime.strptime(edits['timestamp'][index],'%Y-%m-%dT%H:%M:%SZ')\n",
    "                wiki['latest_edit_time'] = datetime.datetime.strftime(edits['timestamp'][0], \"%Y-%m-%d\")\n",
    "                wiki['avg_day_between_edits'] = int(edits['timestamp'].diff(periods=-1).mean()/np.timedelta64(1, 'D'))\n",
    "                edits['year'] = None\n",
    "                for index in edits.index:\n",
    "                    edits['year'][index] = datetime.datetime.strftime(edits['timestamp'][index],'%Y')\n",
    "                wiki['avg_edits_per_year'] = int(edits.groupby('year').size().mean())\n",
    "                try:\n",
    "                    wiki['num_edits_2019'] = len(edits[edits['year'] == '2019'])\n",
    "                except:\n",
    "                    raise\n",
    "                try:\n",
    "                    wiki['num_edits_2018'] = len(edits[edits['year'] == '2018'])\n",
    "                except:\n",
    "                    raise\n",
    "                try:\n",
    "                    wiki['num_edits_2017'] = len(edits[edits['year'] == '2017'])\n",
    "                except:\n",
    "                    raise\n",
    "            except:\n",
    "                raise\n",
    "            # page views\n",
    "            PARAMS3 = {\n",
    "                \"format\": \"json\",\n",
    "                \"action\": \"query\",\n",
    "                \"prop\": \"pageviews\",\n",
    "                \"titles\": search[0],\n",
    "                \"pvipdays\": 60,\n",
    "            }\n",
    "            pageviews = S.get(url=URL, params=PARAMS3).json()\n",
    "            try:\n",
    "                views = pd.DataFrame.from_dict(pageviews['query']['pages'][page_id]['pageviews'], orient='index', columns=['pageviews'])\n",
    "                wiki['pageviews_60d'] = views['pageviews'].sum()\n",
    "            except:\n",
    "                raise\n",
    "            # sections\n",
    "            PARAMS4 = {\n",
    "                \"format\": \"json\",\n",
    "                \"action\": \"parse\",\n",
    "                \"prop\": \"sections\",\n",
    "                \"page\": search[0],\n",
    "            }\n",
    "            sections = S.get(url=URL, params=PARAMS4).json()\n",
    "            try:\n",
    "                wiki['num_sections'] = len(sections['parse']['sections'])\n",
    "            except:\n",
    "                raise\n",
    "            # redirect links\n",
    "            PARAMS5 = {\n",
    "                \"format\": \"json\",\n",
    "                \"action\": \"query\",\n",
    "                \"prop\": \"redirects\",\n",
    "                \"titles\": search[0],\n",
    "            }\n",
    "            redirects = S.get(url=URL, params=PARAMS5).json()\n",
    "            try:\n",
    "                wiki['num_redir_links'] = len(redirects['query']['pages'][page_id]['redirects'])\n",
    "            except:\n",
    "                raise\n",
    "            # references\n",
    "            PARAMS6 = {\n",
    "                \"format\": \"json\",\n",
    "                \"action\": \"parse\",\n",
    "                \"prop\": \"text\",\n",
    "                \"page\": search[0],\n",
    "            }\n",
    "            text = S.get(url=URL, params=PARAMS6).json()\n",
    "            try:\n",
    "                html = bs4.BeautifulSoup(text['parse']['text']['*'])\n",
    "                references = html.find_all('ol', class_=\"references\")\n",
    "                if len(references) == 2:\n",
    "                    wiki['num_references'] = len(references[1].find_all('li'))\n",
    "                else:\n",
    "                    wiki['num_references'] = len(references[0].find_all('li'))\n",
    "            except:\n",
    "                raise\n",
    "            \n",
    "            wikis[i] = wiki\n",
    "        else:\n",
    "            print(i, \"name unmatch\")\n",
    "    except:\n",
    "        print(i, \"no wiki\")\n",
    "# totally 1831 companies have wikipedia page\n",
    "len(wikis)\n",
    "# output the results\n",
    "output = pd.DataFrame(wikis).transpose()\n",
    "# output.to_csv('wikipedia_new.csv')\n",
    "# join with companies list\n",
    "dataset = pd.read_csv('D&B_dataset.csv')\n",
    "dataset = dataset.join(output[['num_edits_2017']], how='left')\n",
    "#dataset.to_excel('D&B_dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
