{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics summary by sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sector:\n",
    "    min_ = []\n",
    "    p1 = []\n",
    "    p5 = []\n",
    "    p10 = []\n",
    "    q1 = []\n",
    "    median_ = []\n",
    "    q3 = []\n",
    "    p90 = []\n",
    "    p95 =[]\n",
    "    p99 = []\n",
    "    max_ = []\n",
    "    mean_ = []\n",
    "    skew = []\n",
    "    kurt = []\n",
    "    kurt += list(complete[complete['Sector'] == i].kurtosis(axis = 0, skipna = True))    \n",
    "    skew += list(complete[complete['Sector'] == i].skew(axis = 0, skipna = True)) \n",
    "    p1 += list(complete[complete['Sector'] == i].quantile(0.01, numeric_only=True)) \n",
    "\n",
    "    p5 += list(complete[complete['Sector'] == i].quantile(0.05, numeric_only=True)) \n",
    "\n",
    "    p10 += list(complete[complete['Sector'] == i].quantile(0.1, numeric_only=True))\n",
    " \n",
    "    p90 += list(complete[complete['Sector'] == i].quantile(0.90, numeric_only=True)) \n",
    "\n",
    "    p95 += list(complete[complete['Sector'] == i].quantile(0.95, numeric_only=True))\n",
    "\n",
    "    p99 += list(complete[complete['Sector'] == i].quantile(0.99, numeric_only=True))\n",
    "\n",
    "    min_ += list(complete[complete['Sector'] == i].min(axis = 0, numeric_only=True))\n",
    "\n",
    "    max_ += list(complete[complete['Sector'] == i].max(axis = 0, numeric_only=True))\n",
    "\n",
    "    mean_ += list(complete[complete['Sector'] == i].mean(axis = 0, numeric_only=True))\n",
    "\n",
    "    median_ += list(complete[complete['Sector'] == i].median(axis = 0, numeric_only=True))\n",
    "\n",
    "    q1 += list(complete[complete['Sector'] == i].quantile(0.25)) \n",
    "\n",
    "    q3 += list(complete[complete['Sector'] == i].quantile(0.75)) \n",
    "\n",
    "    \n",
    "    data0={'variable':variables,'min':min_,'P1':p1,'P5':p5, 'P10':p10, 'Q1':q1, 'median':median_, 'Q3':q3,'P90':p90,\n",
    "          'P95':p95, 'P99':p99,'max':max_, 'mean':mean_,'skew':skew,'kurt':kurt}\n",
    "    data1 = pd.DataFrame(data0)\n",
    "    df1 = complete[complete['Sector'] == i]\n",
    "    zero_ = list()\n",
    "    positive_ = list()\n",
    "    negative_ = list()\n",
    "    miss_ = list()\n",
    "    cov = list()\n",
    "    for n in variables:\n",
    "        zero_.append(len(df1[df1[n] == 0]))\n",
    "        positive_.append(len(df1[df1[n] > 0]))\n",
    "        negative_.append(len(df1[df1[n] < 0]))\n",
    "        miss_.append(len(df1[df1[n].isnull()]))\n",
    "        cov.append(len(df1[df1[n].notnull()]) / len(df1))\n",
    "    data1['zero'] = zero_\n",
    "    data1['positive'] = positive_\n",
    "    data1['negative'] = negative_\n",
    "    data1['missing'] = miss_\n",
    "    data1['coverage'] = cov\n",
    "    \n",
    "    data1.to_csv(f\"summary_{i}_6.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall statistics summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ = []\n",
    "p1 = []\n",
    "p5 = []\n",
    "p10 = []\n",
    "q1 = []\n",
    "median_ = []\n",
    "q3 = []\n",
    "p90 = []\n",
    "p95 =[]\n",
    "p99 = []\n",
    "max_ = []\n",
    "mean_ = []\n",
    "skew = []\n",
    "kurt = []\n",
    "zero_ = list()\n",
    "positive_ = list()\n",
    "negative_ = list()\n",
    "miss_ = list()\n",
    "cov = list()\n",
    "kurt += list(complete.kurtosis(axis = 0, skipna = True))\n",
    "\n",
    "skew += list(complete.skew(axis = 0, skipna = True)) \n",
    "\n",
    "p1 += list(complete.quantile(0.01, numeric_only=True)) \n",
    "\n",
    "p5 += list(complete.quantile(0.05, numeric_only=True)) \n",
    "\n",
    "p10 += list(complete.quantile(0.1, numeric_only=True))\n",
    "\n",
    "p90 += list(complete.quantile(0.90, numeric_only=True)) \n",
    "\n",
    "p95 += list(complete.quantile(0.95, numeric_only=True))\n",
    "\n",
    "p99 += list(complete.quantile(0.99, numeric_only=True))\n",
    "\n",
    "min_ += list(complete.min(axis = 0, numeric_only=True))\n",
    "\n",
    "max_ += list(complete.max(axis = 0, numeric_only=True))\n",
    "\n",
    "mean_ += list(complete.mean(axis = 0, numeric_only=True))\n",
    "\n",
    "median_ += list(complete.median(axis = 0, numeric_only=True))\n",
    "\n",
    "q1 += list(complete.quantile(0.25)) \n",
    "\n",
    "q3 += list(complete.quantile(0.75)) \n",
    "\n",
    "for n in variables:\n",
    "        zero_.append(len(complete[complete[n] == 0]))\n",
    "        positive_.append(len(complete[complete[n] > 0]))\n",
    "        negative_.append(len(complete[complete[n] < 0]))\n",
    "        miss_.append(len(complete[complete[n].isnull()]))\n",
    "        cov.append(len(complete[complete[n].notnull()]) / len(complete))\n",
    "data00={'variable':variables,'coverage':cov,'min':min_,'P1':p1,'P5':p5, 'P10':p10, 'Q1':q1, 'median':median_, 'Q3':q3,'P90':p90,\n",
    "      'P95':p95, 'P99':p99,'max':max_, 'mean':mean_,'skew':skew,'kurt':kurt,'zero':zero_,'positive':positive_,\n",
    "      'negative':negative_, \"missing\":miss_}\n",
    "data2 = pd.DataFrame(data00)\n",
    "data2.to_csv('overall_summary_5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event rate by sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# four category by sector\n",
    "complete= pd.read_excel('D&B dataset.xlsx')\n",
    "columns1 = ['subscriberCount','videoCount','liked','disliked','views','comment','video_2017','video_2018','first']\n",
    "sector = ['Basic Materials', 'Communication Services', 'Consumer Cyclical',\n",
    "       'Consumer Defensive', 'Energy', 'Financial Services','Healthcare', 'Industrials', 'Real Estate', 'Technology', 'Utilities']\n",
    "for i in sector:\n",
    "    youtube1 = complete[complete['Sector'] == i][['subscriberCount','videoCount','liked','disliked','views','comment','video_2017','video_2018','first']]\n",
    "    df1 = pd.read_csv(f\"summary_{i}_6.csv\", index_col=\"variable\").drop('Unnamed: 0', axis = 1)\n",
    "    for a in columns1:\n",
    "        for b in youtube1.index:\n",
    "            if pd.isnull(youtube1[a][b]):\n",
    "                complete[a][b] = 0\n",
    "            elif youtube1[a][b] > df1['P90'][a]:\n",
    "                complete[a][b] = 3\n",
    "            elif youtube1[a][b] < df1['Q1'][a]:\n",
    "                complete[a][b] = 1\n",
    "            else:\n",
    "                 complete[a][b] = 2\n",
    "# export files                    \n",
    "for a in columns1:\n",
    "    grouped = pd.DataFrame(complete.groupby([a, 'Sector','margin_change_90']).size().unstack())\n",
    "    grouped = grouped.replace(np.nan, 0)\n",
    "    grouped['sum'] = grouped[0.0] + grouped[1.0]\n",
    "    grouped['event_rate'] = grouped[1.0]/grouped['sum']\n",
    "    grouped.to_excel(f\"sector_{a}_event rate.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WOE by sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pandas import read_excel\n",
    "complate= pd.read_excel('D&B dataset.xlsx')\n",
    "complete = complete.set_index('ticker')\n",
    "event_rate_sector =read_excel(\"update sector event rate_0410.xlsx\")\n",
    "event_rate = event_rate_sector[['Cate','Sector','subscriberCount','videoCount','liked','disliked','views','comment','video_2017','video_2018','first']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "columns1 = ['subscriberCount','videoCount','liked','disliked','views','comment','video_2017','video_2018','first']\n",
    "sector = ['Basic Materials', 'Communication Services', 'Consumer Cyclical',\n",
    "       'Consumer Defensive', 'Energy', 'Financial Services','Healthcare', 'Industrials', 'Real Estate', 'Technology', 'Utilities']\n",
    "for i in sector:\n",
    "    youtube1 = complete[complete['Sector'] == i][['subscriberCount','videoCount','liked','disliked','views','comment','video_2017','video_2018','first']]\n",
    "    df1 = pd.read_csv(f\"summary_{i}_6.csv\", index_col=\"variable\").drop('Unnamed: 0', axis = 1)\n",
    "    event_rate2 = event_rate[event_rate['Sector'] ==i]\n",
    "    for a in columns1:\n",
    "        for b in youtube1.index:\n",
    "            complete[a][b] = np.nan\n",
    "            if pd.isnull(youtube1[a][b]):\n",
    "                eventrate = event_rate2[event_rate2['Cate'] ==0][a].values[0]\n",
    "                woe = math.log((1-eventrate) / eventrate)\n",
    "                if math.isinf(woe) or math.isnan(woe):\n",
    "                    complete[a][b] = 0\n",
    "                else:\n",
    "                    complete[a][b] = woe\n",
    "            elif youtube1[a][b] > df1['P90'][a]:\n",
    "                eventrate = event_rate2[event_rate2['Cate'] ==3][a].values[0]\n",
    "                woe = math.log((1-eventrate) / eventrate)\n",
    "                if math.isinf(woe) or math.isnan(woe):\n",
    "                    complete[a][b] = 0\n",
    "                else:\n",
    "                    complete[a][b] = woe\n",
    "            \n",
    "            elif youtube1[a][b] < df1['Q1'][a]:\n",
    "                eventrate =  event_rate2[event_rate2['Cate'] ==1][a].values[0]\n",
    "                woe = math.log((1-eventrate) / eventrate)\n",
    "                if math.isinf(woe) or math.isnan(woe):\n",
    "                    complete[a][b] = 0\n",
    "                else:\n",
    "                    complete[a][b] = woe\n",
    "            else:\n",
    "                eventrate = event_rate2[event_rate2['Cate'] ==2][a].values[0]\n",
    "                woe = math.log((1-eventrate) / eventrate)\n",
    "                if math.isinf(woe) or math.isnan(woe):\n",
    "                    complete[a][b] = 0\n",
    "                else:\n",
    "                    complete[a][b] = woe\n",
    "\n",
    "complete.to_excel('woe by sector.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company profile analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High group and low group\n",
    "import pandas as pd\n",
    "from pandas import read_excel\n",
    "data = pd.read_excel('D&B dataset.xlsx')\n",
    "df = pd.read_excel('tree_90_0413.xlsx',sheet_name = 'dataset_leaf')\n",
    "leaf_data = df[['ticker','leaf']]\n",
    "data_1 = pd.merge(data,leaf_data,how = 'inner',on = 'ticker') \n",
    "group_high_plain = data_1[data_1['leaf'] == 13]\n",
    "group_low_plain = data_1[data_1['leaf'] == 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "low_list = ''\n",
    "for i in group_low_plain['Key_Words'].tolist():\n",
    "    low_list += i\n",
    "\n",
    "high_list = ''\n",
    "for i in group_high_plain['Key_Words'].tolist():\n",
    "    high_list += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low group\n",
    "stopwords_low = set(STOPWORDS)\n",
    "stopwords_low.update([\"including\"])\n",
    "wordcloud = WordCloud(stopwords=stopwords_low,max_font_size=50, max_words=80, background_color=\"white\").generate(low_list)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high group\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"including\", \"fixed\", \"approximately\",\"headquartered\"])\n",
    "wordcloud = WordCloud(stopwords=stopwords,max_font_size=50, max_words=80, background_color=\"white\").generate(high_list)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## industry distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([group_high_plain['Sector'], group_low_plain['Sector']], bins=11, label=['High Group', 'Low Group'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Frequency',fontsize=13)\n",
    "plt.xlabel('Industry',fontsize=13)\n",
    "plt.title('Industry Distribution',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## location distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.hist([group_high_plain[group_high_plain['country'] == 'United States']['states'], group_low_plain[group_low_plain['country'] == 'United States']['states']], bins=53, label=['High Group', 'Low Group'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "plt.ylabel('Frequency',fontsize=13)\n",
    "plt.xlabel('Company Location',fontsize=13)\n",
    "plt.title('Company Location Distribution',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## company size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([group_high_plain['number_of_full-time_employee'], group_low_plain['number_of_full-time_employee']], bins='auto', label=['High Group', 'Low Group'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Frequency',fontsize=13)\n",
    "plt.xlabel('Company Size',fontsize=13)\n",
    "plt.title('Company Size Distribution',fontsize=15)\n",
    "plt.xlim(1, 20000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hypothesis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ho: group high = group low\n",
    "# H1: group high != group low\n",
    "from scipy.stats import ttest_ind\n",
    "ttest,pval = ttest_ind(high_diff.dropna()['number_of_full-time_employee'],low_diff.dropna()['number_of_full-time_employee'])\n",
    "pval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
